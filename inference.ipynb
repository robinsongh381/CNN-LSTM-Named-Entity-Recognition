{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils.constant as config\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import glob,re\n",
    "from konlpy.tag import Mecab, Okt, Kkma\n",
    "from models.cnn_lstm import CNNBiLSTM\n",
    "import utils.constant as config\n",
    "\n",
    "\n",
    "device = config.device\n",
    "okt = Okt()\n",
    "word_vocab_dict = torch.load('./data/word_vocab.pt')\n",
    "char_vocab_dict = torch.load('./data/char_vocab.pt')\n",
    "pos_vocab_dict = torch.load('./data/pos_vocab.pt')\n",
    "entitiy_to_index = torch.load('./data/processed_data/entity_to_index.pt')\n",
    "num_class = len(entitiy_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary():\n",
    "    def __init__(self):\n",
    "        self.vocab = set()\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "        # self.add_special()\n",
    "        \n",
    "    def add_special(self):\n",
    "        special_tokens = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
    "        for word in special_tokens:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.vocab.add(word)\n",
    "            self.idx += 1\n",
    "        \n",
    "    def add_word(self, tokenized_text, char=False, pos_tag=False):\n",
    "        for word in tokenized_text:\n",
    "            \n",
    "            if not char and not pos_tag:\n",
    "                if word not in self.vocab:\n",
    "                    self.word2idx[word] = self.idx\n",
    "                    self.idx2word[self.idx] = word\n",
    "                    self.vocab.add(word)\n",
    "                    self.idx += 1\n",
    "                    \n",
    "            elif char and not pos_tag:\n",
    "                for c in word:\n",
    "                    if c not in self.vocab:\n",
    "                        self.word2idx[c] = self.idx\n",
    "                        self.idx2word[self.idx] = c\n",
    "                        self.vocab.add(c)\n",
    "                        self.idx += 1\n",
    "                        \n",
    "            elif not char and pos_tag:\n",
    "                for pos in word[1]:\n",
    "                    if pos not in self.vocab:\n",
    "                        self.word2idx[pos] = self.idx\n",
    "                        self.idx2word[self.idx] = pos\n",
    "                        self.vocab.add(pos)\n",
    "                        self.idx += 1\n",
    "                            \n",
    "    def convert_tokens_to_idx(self, list_of_tokens, add_special=False):\n",
    "        list_of_idx = []\n",
    "        for w in list_of_tokens:\n",
    "            try:\n",
    "                idx = self.word2idx[w]\n",
    "            except:\n",
    "                idx = self.word2idx['<unk>']\n",
    "            list_of_idx.append(idx)\n",
    "            \n",
    "        return list_of_idx\n",
    " \n",
    "    def convert_chars_to_idx(self, list_of_tokens, add_special=False):\n",
    "        list_of_idx = []\n",
    "        for w in list_of_tokens:\n",
    "            char= []\n",
    "            for c in w:\n",
    "                try:\n",
    "                    idx = self.word2idx[c]\n",
    "                except:\n",
    "                    idx = self.word2idx['<unk>']\n",
    "                char.append(idx)\n",
    "            list_of_idx.append(char)\n",
    "            \n",
    "        return list_of_idx\n",
    "    \n",
    "    def convert_pos_to_idx(self, raw_text, add_special=False):\n",
    "        list_of_idx = []\n",
    "        pos_tag = okt.pos(raw_text)\n",
    "\n",
    "        for p in pos_tag:\n",
    "            pos = p[1]\n",
    "            try:\n",
    "                idx = self.word2idx[pos]\n",
    "            except:\n",
    "                idx = self.word2idx['<unk>']\n",
    "            list_of_idx.append(idx)\n",
    "            \n",
    "        return list_of_idx\n",
    "                    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(raw_text):\n",
    "    return okt.morphs(raw_text)\n",
    "\n",
    "\n",
    "def convert_to_label(pred_label):\n",
    "    def get_idx_to_entity():\n",
    "        idx2ent = {}\n",
    "        for k,v in entitiy_to_index.items():\n",
    "            idx2ent[v]=k\n",
    "        return idx2ent\n",
    "    idx_to_entity = get_idx_to_entity()\n",
    "    tag = [idx_to_entity[i] for i in pred_label]\n",
    "\n",
    "    return tag\n",
    "\n",
    "\n",
    "def load_model(model, path):\n",
    "    model_files = glob.glob(model_save_path+'/*.pt')\n",
    "    best_model = model_files[0]\n",
    "    ckpt = torch.load(best_model)\n",
    "    model.load_state_dict(ckpt['model_state_dict'])\n",
    "    model.eval().to(device)\n",
    "    print('Loading checkpoint from {}'.format(best_model))\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_vocab(vocab_name):\n",
    "    vocab = Vocabulary()\n",
    "    \n",
    "    if vocab_name =='word' or vocab_name=='token':\n",
    "        vocab.word2idx = word_vocab_dict\n",
    "    elif vocab_name =='char':\n",
    "        vocab.word2idx = char_vocab_dict\n",
    "    elif vocab_name == 'pos':\n",
    "        vocab.word2idx = pos_vocab_dict\n",
    "    else:\n",
    "        raise Exception('Unknwon vocab type')\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def pad_char_idx(char_idx, token_len):\n",
    "    maxlen = max(token_len, max(len(i) for i in char_idx))\n",
    "    padded = []\n",
    "    for char in char_idx:\n",
    "        char += [0]*(maxlen-len(char))\n",
    "        padded.append(char)\n",
    "    return padded\n",
    "\n",
    "\n",
    "def transform_to_model_input(text):\n",
    "    \n",
    "    token_vocab, char_vocab, pos_vocab = load_vocab('token'), load_vocab('char'), load_vocab('pos')\n",
    "    \n",
    "    def _to_tensor(x):\n",
    "        return torch.tensor(x).long().unsqueeze(0).to(device)\n",
    "\n",
    "    tokenized_text = tokenizer(text)\n",
    "    token_idx = token_vocab.convert_tokens_to_idx(tokenized_text)\n",
    "    char_idx = char_vocab.convert_chars_to_idx(tokenized_text)\n",
    "    pos_idx = pos_vocab.convert_pos_to_idx(text)\n",
    "    \n",
    "    # Pad & Tensor\n",
    "    token_tensor = _to_tensor(token_idx)\n",
    "    char_tensor = _to_tensor(pad_char_idx(char_idx, len(token_idx)))\n",
    "    pos_tensor = _to_tensor(pos_idx)\n",
    "    \n",
    "    return token_tensor, char_tensor, pos_tensor\n",
    "\n",
    "\n",
    "def get_entity(token, pred):\n",
    "    answer = []\n",
    "    \n",
    "    token = tokenizer(token)\n",
    "    assert len(token)==len(pred)\n",
    "\n",
    "    answer = []\n",
    "    for i in range(len(token)):\n",
    "        if pred[i][0]=='B':  \n",
    "            pref, suf = pred[i].split('-')[0], pred[i].split('-')[1]\n",
    "            value=token[i]\n",
    "\n",
    "            try:\n",
    "                for j in range(i+1, len(pred)):\n",
    "                    if pred[j]=='I-'+suf:\n",
    "                        value += token[i+1]\n",
    "            except:\n",
    "                break\n",
    "            answer.append((value, suf))     \n",
    "            \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ./result/epoch_60_batch_64_ch_in_1_ch_out_32/epoch_55_step_19745_tr_acc_0.431_eval_acc_0.891.pt\n"
     ]
    }
   ],
   "source": [
    "model = CNNBiLSTM(config, num_class, len(word_vocab_dict), len(char_vocab_dict), len(pos_vocab_dict))\n",
    "model_save_path='./result/epoch_60_batch_64_ch_in_1_ch_out_32'\n",
    "\n",
    "model = load_model(model, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['다음주', '수요일', '인천', '국제공항', '에서', '5시', '30분', '에', '8', '인승', '으로', '예약', '해주세요', ',', '이름', '은', '김진수', '입니다']\n",
      "['O', 'B-DAT', 'B-LOC', 'I-LOC', 'O', 'B-NOH', 'I-NOH', 'O', 'B-NOH', 'I-NOH', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PER', 'O']\n",
      " \n",
      "[('수요일', 'DAT'), ('인천국제공항', 'LOC'), ('5시30분30분', 'NOH'), ('8인승', 'NOH'), ('김진수', 'PER')]\n"
     ]
    }
   ],
   "source": [
    "text = '다음주 수요일 인천국제공항에서 5시30분에 8인승으로 예약해주세요, 이름은 김진수입니다'\n",
    "\n",
    "print(tokenizer(text))\n",
    "token_tensor, char_tensor, pos_tensor = transform_to_model_input(text)\n",
    "logit = model(token_tensor, char_tensor, pos_tensor)\n",
    "\n",
    "# To Labels\n",
    "pred_label = logit.argmax(-1)\n",
    "pred_label_list = pred_label.tolist()[0]\n",
    "result = convert_to_label(pred_label_list)\n",
    "print(result)\n",
    "print(' ')\n",
    "\n",
    "# Answer\n",
    "ans = get_entity(text, result)\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatorch",
   "language": "python",
   "name": "chatorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
